{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09d5da7",
   "metadata": {},
   "source": [
    "# Part 2: Data Preparation\n",
    " ----\n",
    "\n",
    "Note this Demo is based on ngc docker image `nvcr.io/nvidia/pytorch:21.11-py3`\n",
    "\n",
    "This notebook walks you each step to train a model using containers from the NGC Catalog. We chose the GPU optimized Pytorch container as an example. The basics of working with docker containers apply to all NGC containers.\n",
    "\n",
    "We will show you how to:\n",
    "\n",
    "* Download the Xview Dataset\n",
    "* How to convert labels to coco format\n",
    "* How to conduct the preprocessing step tiling (i.e. slicing large satellite imagery into chunks )\n",
    "* How to upload to s3 bucket to support distributed training\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Pre-reqs, set up jupyter notebook environment using NGC container \n",
    "\n",
    "# Execute docker run to create NGC environment for Data Prep\n",
    "make sure to map host directory to docker directory, we will use the host directory again to \n",
    "* `docker run   --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/ubuntu:/home/ubuntu  -p 8008:8888 -it nvcr.io/nvidia/pytorch:21.11-py3  /bin/bash`\n",
    "\n",
    "# Run jupyter notebook command within docker container to access it on your local browser\n",
    "* `cd /home/ubuntu`\n",
    "* `jupyter lab --ip=0.0.0.0 --port=8888 --NotebookApp.token='' --NotebookApp.password=''` \n",
    "* `git clone https://github.com/interactivetech/e2e_blogposts.git`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48093cbd",
   "metadata": {},
   "source": [
    "### Download the Xview Dataset\n",
    "The dataset we will be using is from the DIUx xView 2018 Challenge https://challenge.xviewdataset.org by U.S. National Geospatial-Intelligence Agency (NGA). You will need to create an account at https://challenge.xviewdataset.org/welcome, agree to the terms and conditions, and download the dataset manually.\n",
    "\n",
    "You can download the dataset at the url https://challenge.xviewdataset.org/data-download\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8276118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sahi\n",
      "  Downloading sahi-0.11.14-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.8/site-packages (0.20.0)\n",
      "Collecting opencv-python-headless==4.5.5.64\n",
      "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from opencv-python-headless==4.5.5.64) (1.24.3)\n",
      "Requirement already satisfied: pillow>=8.2.0 in /opt/conda/lib/python3.8/site-packages (from sahi) (9.5.0)\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m199.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting shapely>=1.8.0\n",
      "  Downloading shapely-2.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from sahi) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /opt/conda/lib/python3.8/site-packages (from sahi) (4.65.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.4-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python>=4.2.0.32\n",
      "  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from sahi) (2.29.0)\n",
      "Collecting terminaltables\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pybboxes==0.1.6\n",
      "  Downloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: scipy<1.9.2,>=1.8 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.9.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2.28.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (23.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (0.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from fire->sahi) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from fire->sahi) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (2.0.4)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=06d33e346c3a9584c13a696b77db025225e081a95d4bbd4273ea7fb904b60b4c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wa6zpb8h/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
      "Successfully built fire\n",
      "Installing collected packages: terminaltables, shapely, pybboxes, opencv-python-headless, opencv-python, fire, click, sahi\n",
      "Successfully installed click-8.1.4 fire-0.5.0 opencv-python-4.8.0.74 opencv-python-headless-4.5.5.64 pybboxes-0.1.6 sahi-0.11.14 shapely-2.0.1 terminaltables-3.1.10\n"
     ]
    }
   ],
   "source": [
    "# run pip install to get the SAHI library\n",
    "!pip install sahi scikit-image opencv-python-headless==4.5.5.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c9f059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-11 21:19:10--  https://d307kc0mrhucc3.cloudfront.net/train_images.tgz?Expires=1689131216&Signature=PC5FQU8ls3vvBIxeOLcl-GgeYnidq~qolQymEfeENmxvpW~D6eo6O0rtCm4D4O4EBkzMMIUJSeofrHx09GNf2cgPbTW3LTN8fIgN4UaRNCeLWVwHj7wC5DHyoDaCsN7-G3Z7jlslXiPLR8u4DaqlI-h4-vtR1UzxfWjoH3wtOe7GeTcSnINbdtc88MtqbSofh6FOIpRZ9XrhcpQ8fv43cQKLsCZLAR48Jg56ByoXWoXVoCrtcbviX67lyfa0YicvnbS9Ji6EKk8scBeE~LMfZ3KvyXlxlpJankKFlh5pv5P25ocKYOnlKmzMM-cdWL3JQr6-GWp4~pzyPMsMQJRheQ__&Key-Pair-Id=APKAIKGDJB5C3XUL2DXQ\n",
      "Resolving d307kc0mrhucc3.cloudfront.net (d307kc0mrhucc3.cloudfront.net)... 18.161.153.4, 18.161.153.12, 18.161.153.53, ...\n",
      "Connecting to d307kc0mrhucc3.cloudfront.net (d307kc0mrhucc3.cloudfront.net)|18.161.153.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15413902447 (14G) [application/gzip]\n",
      "Saving to: ‘train_images.tgz’\n",
      "\n",
      "train_images.tgz    100%[===================>]  14.35G  55.4MB/s    in 5m 20s  \n",
      "\n",
      "2023-07-11 21:24:30 (45.9 MB/s) - ‘train_images.tgz’ saved [15413902447/15413902447]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example command to download train images with wget command, you will need to update the url as the token is expired\"\n",
    "!wget -O train_images.tgz \\\n",
    "  \"https://d307kc0mrhucc3.cloudfront.net/train_images.tgz?Expires=1689131216&Signature=PC5FQU8ls3vvBIxeOLcl-GgeYnidq~qolQymEfeENmxvpW~D6eo6O0rtCm4D4O4EBkzMMIUJSeofrHx09GNf2cgPbTW3LTN8fIgN4UaRNCeLWVwHj7wC5DHyoDaCsN7-G3Z7jlslXiPLR8u4DaqlI-h4-vtR1UzxfWjoH3wtOe7GeTcSnINbdtc88MtqbSofh6FOIpRZ9XrhcpQ8fv43cQKLsCZLAR48Jg56ByoXWoXVoCrtcbviX67lyfa0YicvnbS9Ji6EKk8scBeE~LMfZ3KvyXlxlpJankKFlh5pv5P25ocKYOnlKmzMM-cdWL3JQr6-GWp4~pzyPMsMQJRheQ__&Key-Pair-Id=APKAIKGDJB5C3XUL2DXQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4d1913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-11 21:24:31--  https://d307kc0mrhucc3.cloudfront.net/train_labels.tgz?Expires=1689131216&Signature=DuWDhUvne4g9Mp~KbK~9VJdfrUybAKusLwXoGFPZ43D86y2bSV3BY08PNaMooENOFiJFlqVsXPSp512ZxxiITakSQ889YEgHKxDHPiMyO4OCILWZYmpivTrw3AI3gYQXCAMwkz3v~1WrgX2y8Yi5VTCtrNKWXgYFyOULCQCD6gJFJX7Buq0ldwY7nQQXoaqf2vYO7LKCviHt3EK6-CtO3sRB82LLmqLK8x~Sau~HM06v40s8jnBbU8m~W81zqQh5LMziBz7suAYeVNv8hhE5ej6IXJ9JgIatrhE8Ki9ytdWNxFTDokQUqW7DPioeGDMRfeu1xCuojxVbfBLtGhyeaQ__&Key-Pair-Id=APKAIKGDJB5C3XUL2DXQ\n",
      "Resolving d307kc0mrhucc3.cloudfront.net (d307kc0mrhucc3.cloudfront.net)... 18.161.153.53, 18.161.153.129, 18.161.153.4, ...\n",
      "Connecting to d307kc0mrhucc3.cloudfront.net (d307kc0mrhucc3.cloudfront.net)|18.161.153.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 48950328 (47M) [application/gzip]\n",
      "Saving to: ‘train_labels.tgz’\n",
      "\n",
      "train_labels.tgz    100%[===================>]  46.68M  54.8MB/s    in 0.9s    \n",
      "\n",
      "2023-07-11 21:24:32 (54.8 MB/s) - ‘train_labels.tgz’ saved [48950328/48950328]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example command to download train images with wget command, you will need to update the url as the token is expired\"\n",
    "!wget -O train_labels.tgz \\\n",
    "  \"https://d307kc0mrhucc3.cloudfront.net/train_labels.tgz?Expires=1689131216&Signature=DuWDhUvne4g9Mp~KbK~9VJdfrUybAKusLwXoGFPZ43D86y2bSV3BY08PNaMooENOFiJFlqVsXPSp512ZxxiITakSQ889YEgHKxDHPiMyO4OCILWZYmpivTrw3AI3gYQXCAMwkz3v~1WrgX2y8Yi5VTCtrNKWXgYFyOULCQCD6gJFJX7Buq0ldwY7nQQXoaqf2vYO7LKCviHt3EK6-CtO3sRB82LLmqLK8x~Sau~HM06v40s8jnBbU8m~W81zqQh5LMziBz7suAYeVNv8hhE5ej6IXJ9JgIatrhE8Ki9ytdWNxFTDokQUqW7DPioeGDMRfeu1xCuojxVbfBLtGhyeaQ__&Key-Pair-Id=APKAIKGDJB5C3XUL2DXQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32be7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip images and labels from /home/ubuntu/e2e_blogposts/ngc_blog\n",
    "!tar -xf train_images.tgz -C xview_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edcfb48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip labels from /home/ubuntu/e2e_blogposts/ngc_blog directory \n",
    "!tar -xf train_labels.tgz -C xview_dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa0f87",
   "metadata": {},
   "source": [
    "# Convert TIF to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6b1d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created xview_dataset/train_images_rgb/ ...\n",
      "renaming bad named files...\n",
      "[PosixPath('xview_dataset/train_images/._109.tif'), PosixPath('xview_dataset/train_images/._102.tif'), PosixPath('xview_dataset/train_images/._100.tif')]\n",
      "[PosixPath('109.tif'), PosixPath('102.tif'), PosixPath('100.tif')]\n",
      "100%|█████████████████████████████████████████| 846/846 [55:13<00:00,  3.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Here loop through all the images and convert them to RGB, this is \n",
    "# important for tiling the images and training with pytorch\n",
    "# will take about an hour to complete\n",
    "!python data_utils/tif_2_rgb.py --input_dir xview_dataset/train_images \\\n",
    "  --out_dir xview_dataset/train_images_rgb/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e04057",
   "metadata": {},
   "source": [
    "# How to convert labels to coco format\n",
    "Here we run a script to convert the dataset labels from .geojson format to COCO format. More details on the COCO format here: \n",
    "\n",
    "The result will be two files (in COCO formal) generated `train.json` and `val.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8fdacf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(category_id_remapping='data_utils/category_id_mapping.json', output_dir='xview_dataset/', train_geojson_path='xview_dataset/xView_train.geojson', train_images_dir='xview_dataset/train_images/', train_images_dir_rgb='xview_dataset/train_images_rgb/', train_split_rate=0.75, xview_class_labels='data_utils/xview_class_labels.txt')\n",
      "5.tif:  True\n",
      "Parsing xView data: 100%|████████████| 601937/601937 [00:07<00:00, 78172.18it/s]\n",
      "Converting xView data into COCO format: 100%|█| 846/846 [01:32<00:00,  9.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# make sure train_images_dir is pointing to the .tif images\n",
    "!python data_utils/convert_geojson_to_coco.py --train_images_dir xview_dataset/train_images/ \\\n",
    "  --train_images_dir_rgb xview_dataset/train_images_rgb/ \\\n",
    "  --train_geojson_path xview_dataset/xView_train.geojson \\\n",
    "  --output_dir xview_dataset/ \\\n",
    "  --train_split_rate 0.75 \\\n",
    "  --category_id_remapping data_utils/category_id_mapping.json \\\n",
    "  --xview_class_labels data_utils/xview_class_labels.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756bea5",
   "metadata": {},
   "source": [
    "# Slicing/Tiling the Dataset\n",
    "Here we are using the SAHI library to slice our large satellite images. Satellite images can be up to 50k^2 pixels in size, which wouldnt fit in GPU memory. We alleviate this problem by slicing the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ae4acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing step is starting...\n",
      "indexing coco dataset annotations...\n",
      "Loading coco annotations: 100%|███████████████| 634/634 [00:28<00:00, 22.13it/s]\n",
      "100%|█████████████████████████████████████████| 634/634 [13:36<00:00,  1.29s/it]\n",
      "Sliced dataset for 'slice_size: 640' is exported to xview_dataset/train_images_rgb_no_neg/\n",
      "Slicing step is starting...\n",
      "indexing coco dataset annotations...\n",
      "Loading coco annotations: 100%|███████████████| 212/212 [00:07<00:00, 26.97it/s]\n",
      "100%|█████████████████████████████████████████| 212/212 [04:01<00:00,  1.14s/it]\n",
      "Sliced dataset for 'slice_size: 640' is exported to xview_dataset/val_images_rgb_no_neg/\n"
     ]
    }
   ],
   "source": [
    "!python data_utils/slice_coco.py --image_dir xview_dataset/train_images_rgb/ \\\n",
    "  --train_dataset_json_path xview_dataset/train.json \\\n",
    "  --val_dataset_json_path xview_dataset/val.json \\\n",
    "  --slice_size 640 \\\n",
    "  --overlap_ratio 0.2 \\\n",
    "  --ignore_negative_samples True \\\n",
    "  --min_area_ratio 0.1 \\\n",
    "  --output_train_dir xview_dataset/train_images_rgb_no_neg/ \\\n",
    "  --output_val_dir xview_dataset/val_images_rgb_no_neg/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c9460",
   "metadata": {},
   "source": [
    "# Upload to s3 bucket to support distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0333911-190a-4e30-9285-c30638b05eca",
   "metadata": {},
   "source": [
    "We will now upload our exported data to a publically accessible S3 bucket. This will enable for a large scale distributed experiment to have access to the dataset without installing the dataset on device. \n",
    "View these links to learn how to upload your dataset to an S3 bucket. Review the `S3Backend` class in `data.py`\n",
    "* https://docs.determined.ai/latest/training/load-model-data.html#streaming-from-object-storage\n",
    "* https://codingsight.com/upload-files-to-aws-s3-with-the-aws-cli/\n",
    "\n",
    "Once you create an S3 bucket that is publically accessible, here are example commands to upload the preprocessed dataset to S3:\n",
    "* `aws s3 cp --recursive xview_dataset/train_sliced_no_neg/   s3://determined-ai-xview-coco-dataset/train_sliced_no_neg`\n",
    "* `aws s3 cp --recursive xview_dataset/val_sliced_no_neg/   s3://determined-ai-xview-coco-dataset/val_sliced_no_neg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee38dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
