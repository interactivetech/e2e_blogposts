{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a16f4ce",
   "metadata": {},
   "source": [
    "# Part 3: End-to-End Example training object detection model using NVIDIA Pytorch Container from NGC\n",
    "## Training and Inference via NGC Container\n",
    " ----\n",
    "\n",
    "\n",
    "\n",
    "This notebook walks you each step to train a model using containers from the NGC Catalog. We chose the GPU optimized Pytorch container as an example. The basics of working with docker containers apply to all NGC containers.\n",
    "\n",
    "We will show you how to:\n",
    "\n",
    "* Execute training a object detection on satellite imagery using TensorFlow and Jupyter Notebook\n",
    "* Run inference on a trained object detection model using the SAHI library\n",
    "\n",
    "Note this Object Detection demo is based on https://github.com/pytorch/vision/tree/v0.11.3 and ngc docker image `nvcr.io/nvidia/pytorch:21.11-py3`\n",
    "\n",
    "We assume you completed step 2 of dataset preprocessing and have your tiled satellite imagery dataset completed and in the local directory `train_images_rgb_no_neg/train_images_300_02`\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "# Execute docker run to create NGC environment for Data Prep\n",
    "make sure to map host directory to docker directory, we will use the host directory again to \n",
    "* `docker run   --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/ubuntu:/home/ubuntu  -p 8008:8888 -it nvcr.io/nvidia/pytorch:21.11-py3  /bin/bash`\n",
    "\n",
    "# Run jupyter notebook command within docker container to access it on your local browser\n",
    "* `cd /home/ubuntu`\n",
    "* `jupyter lab --ip=0.0.0.0 --port=8888 --NotebookApp.token='' --NotebookApp.password=''` \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85880ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install cython pycocotools matplotlib terminaltables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a22b7c",
   "metadata": {},
   "source": [
    "# TLDR; run training job on 4 GPUS\n",
    "The below cell will run a multi-gpu training job. This job will train an object detection model (faster-rcnn) on a dataset of satellite imagery images that contain 61 classes of objects\n",
    "* Change `nproc_per_node` argument to specify the number of GPUs available on your server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=4 detection/train.py\\\n",
    "    --dataset coco --data-path=xview_dataset/ --model fasterrcnn_resnet50_fpn --epochs 26\\\n",
    "    --lr-steps 16 22 --aspect-ratio-group-factor 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410884d8-dae3-4436-85cc-6e0aee7164d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"detection/train.py\", line 34, in <module>\n",
      "    from models import build_frcnn_model\n",
      "  File \"/run/determined/workdir/shared_fs/01 - Users/andrew.mendez/e2e_blogposts/ngc_blog/detection/models.py\", line 13\n",
      "    model.load_state_dict(torch.load(,map_location=torch.device('cpu')))\n",
      "                                     ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python detection/train.py\\\n",
    "    --dataset coco --data-path=xview_dataset/ --model fasterrcnn_resnet50_fpn --epochs 1\\\n",
    "    --lr-steps 16 22 --aspect-ratio-group-factor 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8d0f4",
   "metadata": {},
   "source": [
    "### Object Detection on Satellite Imagery with Pytorch (Single GPU)\n",
    "Follow and Run the code to train a Faster RCNN FPN (Resnet50 backbone) that classifies images of clothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python dependencies\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "from coco_utils import get_coco, get_coco_kp\n",
    "\n",
    "from group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "import presets\n",
    "import utils\n",
    "from coco_utils import get_coco, get_coco_kp\n",
    "from train import get_dataset, get_transform\n",
    "from group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from engine import train_one_epoch, evaluate\n",
    "from models import build_frcnn_model\n",
    "from PIL import Image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from vis_utils import load_determined_state_dict, visualize_pred, visualize_gt, predict\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c56900",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='output'\n",
    "data_path='xview_dataset/'\n",
    "dataset_name='coco'\n",
    "model_name='fasterrcnn_resnet50_fpn'\n",
    "device='cpu'\n",
    "batch_size=8\n",
    "epochs=26\n",
    "workers=4\n",
    "lr=0.02\n",
    "momentum=0.9\n",
    "weight_decay=1e-4\n",
    "lr_scheduler='multisteplr'\n",
    "lr_step_size=8\n",
    "lr_steps=[16, 22]\n",
    "lr_gamma=0.1\n",
    "print_freq=20\n",
    "resume=False\n",
    "start_epoch=0\n",
    "aspect_ratio_group_factor=3\n",
    "rpn_score_thresh=None\n",
    "trainable_backbone_layers=None\n",
    "data_augmentation='hflip'\n",
    "pretrained=True\n",
    "test_only=False\n",
    "sync_bn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad34f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset.\n",
    "# Data loading code\n",
    "print(\"Loading data\")\n",
    "\n",
    "dataset, num_classes = get_dataset(dataset_name, \n",
    "                                   \"train\", \n",
    "                                   get_transform(True, data_augmentation),\n",
    "                                   data_path)\n",
    "dataset_test, _ = get_dataset(dataset_name, \n",
    "                              \"val\", \n",
    "                              get_transform(False, data_augmentation),\n",
    "                              data_path)\n",
    "print(dataset.num_classes)\n",
    "print(\"Creating data loaders\")\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "train_batch_sampler = torch.utils.data.BatchSampler(\n",
    "            train_sampler, batch_size, drop_last=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_sampler=train_batch_sampler, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1,\n",
    "    sampler=test_sampler, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting three examples from the test dataset\n",
    "inds_that_have_boxes = []\n",
    "test_images = list(data_loader_test)\n",
    "for ind,(im,targets) in tqdm(enumerate(test_images),total=len(list(data_loader_test))):\n",
    "    # print(ind,targets)\n",
    "    if targets[0]['boxes'].shape[0]>0:\n",
    "        # print(targets[0]['boxes'].shape[0])\n",
    "        # print(ind,targets)\n",
    "        inds_that_have_boxes.append(ind)\n",
    "\n",
    "images_t_list=[]\n",
    "targets_t_list=[]\n",
    "for ind in tqdm(range(3)):\n",
    "    im,targets = test_images[ind]\n",
    "    images_t_list.append(im[0])\n",
    "    targets_t_list.append(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d78a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at one of the images. The following code visualizes\n",
    "# the images using the matplotlib library.\n",
    "im = Image.fromarray(\n",
    "    (\n",
    "    255.*images_t_list[0].cpu().permute((1,2,0)).numpy()).astype(np.uint8)\n",
    "    )\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35942ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look again at the first three images, but this time with the class names.\n",
    "\n",
    "for i,t in zip(images_t_list,targets_t_list):\n",
    "    im = Image.fromarray((255.*i.cpu().permute((1,2,0)).numpy()).astype(np.uint8))\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    im = visualize_gt(i,t)\n",
    "    plt.imshow(im)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db266e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the model:\n",
    "print(\"Creating model\")\n",
    "print(\"Number of classes: \",dataset.num_classes)\n",
    "model = build_frcnn_model(num_classes=dataset.num_classes)\n",
    "_=model.to('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model:\n",
    "# Define loss function, optimizer, and metrics.\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = lr_scheduler.lower()\n",
    "if lr_scheduler == 'multisteplr':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=lr_steps, \n",
    "                                                        gamma=lr_gamma)\n",
    "elif lr_scheduler == 'cosineannealinglr':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "else:\n",
    "    raise RuntimeError(\"Invalid lr scheduler '{}'. Only MultiStepLR and CosineAnnealingLR \"\n",
    "                       \"are supported.\".format(args.lr_scheduler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "# Let's train 1 epoch. \n",
    "# After every epoch, training time, loss, and accuracy will be displayed.\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq)\n",
    "    lr_scheduler.step()\n",
    "    if output_dir:\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'args': args,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'model_{}.pth'.format(epoch)))\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'checkpoint.pth'))\n",
    "\n",
    "    # evaluate after every epoch\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frcnn_model2(num_classes):\n",
    "    print(\"Loading pretrained model...\")\n",
    "    # load an detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.min_size=800\n",
    "    model.max_size=1333\n",
    "    # RPN parameters\n",
    "    model.rpn_pre_nms_top_n_train=2000\n",
    "    model.rpn_pre_nms_top_n_test=1000\n",
    "    model.rpn_post_nms_top_n_train=2000\n",
    "    model.rpn_post_nms_top_n_test=1000\n",
    "    model.rpn_nms_thresh=1.0\n",
    "    model.rpn_fg_iou_thresh=0.7\n",
    "    model.rpn_bg_iou_thresh=0.3\n",
    "    model.rpn_batch_size_per_image=256\n",
    "    model.rpn_positive_fraction=0.5\n",
    "    model.rpn_score_thresh=0.05\n",
    "    # Box parameters\n",
    "    model.box_score_thresh=0.0\n",
    "    model.box_nms_thresh=1.0\n",
    "    model.box_detections_per_img=500\n",
    "    model.box_fg_iou_thresh=1.0\n",
    "    model.box_bg_iou_thresh=1.0\n",
    "    model.box_batch_size_per_image=512\n",
    "    model.box_positive_fraction=0.25\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the model performs on the test data:\n",
    "# make sure to wait 8 epochs to load model_8.pth\n",
    "model = build_frcnn_model2(num_classes=61)\n",
    "ckpt = torch.load('model_8.pth',map_location='cpu')\n",
    "model.load_state_dict(ckpt['model'])\n",
    "_=model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d79a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=predict(model,images_t_list,targets_t_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
